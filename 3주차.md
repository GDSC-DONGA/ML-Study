# MLP(Multi Layer Perceptron)
여러 층으로 구성된 perceptron 모델   
입력층, 은닉층, 출력층으로 구성됨


<img
  src="https://wikidocs.net/images/page/24958/%ED%8D%BC%EC%85%89%ED%8A%B8%EB%A1%A03.PNG"
  width="230"
  height="200"
/>
<img
  src="https://wikidocs.net/images/page/24958/%EC%9E%85%EC%9D%80%EC%B8%B5.PNG"
  width="300"
  height="200"
/>

각 입력(x)에 대응되는 weight(w), 1개의 노드에 입력되는 bias(b) 존재   
가중합으로 얻어진 결과치에 **활성화 함수(h)** 적용
> 활성화 함수 : 입력 값들의 가중 합을 통해 노드의 활성화 여부를 판단하는 함수

<img     
     src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fk.kakaocdn.net%2Fdn%2FbLK9zl%2FbtrhTSSogiz%2FA3VOnIrCyQcd2lsll9k9b1%2Fimg.png"
  width="350"
  height="200"
/>

전 결합 계층(Fully Connected layer)
각 층별로 모든 노드가 연결되어 weight를 가지는 계층

---
### 활성화 함수(Activation function)
- **Sigmoid function**

0~1 사이 실수를 출력 > 확률로 해석 가능   
기울기가 발생하지 않는 지점 존재   

<img
  src="https://wikidocs.net/images/page/60683/%EC%8B%9C%EA%B7%B8%EB%AA%A8%EC%9D%B4%EB%93%9C%ED%95%A8%EC%88%981.PNG"
  width="300"
  height="200"
/>


- **Rectified Linear Unit(ReLU) function**

0 이하면 0 / 0보다 크면 값을 그대로 출력   
단순한 연산 > 학습 속도 빠릅   
입력 값이 0이하인 경우 기울기는 항상 0

<img
  src="https://wikidocs.net/images/page/60683/%EB%A0%90%EB%A3%A8%ED%95%A8%EC%88%98.PNG"
  width="300"
  height="200"
/>

